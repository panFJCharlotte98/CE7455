{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqwYZsMoPKyV"
      },
      "source": [
        "# Skip Gram in PyTorch\n",
        "\n",
        "- [Ref](https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296)\n",
        "\n",
        "- [Code](https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/02.Skip-gram-Negative-Sampling.ipynb)\n",
        "\n",
        "- Edited by Han Cheol Moon\n",
        "\n",
        "![skipgram](https://cdn-images-1.medium.com/max/800/1*SR6l59udY05_bUICAjb6-w.png)\n",
        "\n",
        "- Skip-gram’s objective is to predict the contexts, given a target word: $V_t \\rightarrow V_c$\n",
        "- The contexts are immediate neighbours of the target and are retrieved using a window of an arbitrary size $n$\n",
        "    - Capturing $n$ words to the left of the target and $n$ words to its right.\n",
        "- In a two-gram example:\n",
        "\n",
        "$$\\underbrace{\\textrm{The quick}}_{\\textrm{left } n}\\underbrace{\\textrm{ brown }}_{target} \\underbrace{\\textrm{for jumps}}_{\\textrm{right } n}$$\n",
        "\n",
        "<img src=\"https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/images/01.skipgram-prepare-data.png\">\n",
        "\n",
        "- The original Skip-gram's objective is to maximise $P(V_c|V_t)$: The probability of $V_c$ being predicted as $V_t$’s context for all training pairs.\n",
        "\n",
        "- To calculate $P(V_c|V_t)$ we need a way to quantify the __closeness__ of the target-word and the context-word.\n",
        "- In Skip-gram, this closeness is computed using the __dot product between the input-embedding of the target and the output-embedding of the context__.\n",
        "\n",
        "Now, if we define $u_{t,c}$ to be the measure of words' closeness between the target word and context word, $E$ to be the embedding matrix holding input-embeddings and $O$ to be the output-embedding matrix we get:\n",
        "\n",
        "$$\\large u_{t,c} = E_t \\cdot O_c$$\n",
        "\n",
        ", where $c$ is the context and $t$ is the target. With softmax,\n",
        "\n",
        "![Skipgram](https://cdn-images-1.medium.com/max/1600/1*4Viy_LvP6jLIWSvB9-Fk-Q.png)\n",
        "\n",
        "\\begin{equation}\\large \\prod P(V_c|V_t) \\rightarrow \\sum logP(V_c|V_t) \\rightarrow \\sum log\\frac{\\exp^{u_{t,c}}}{\\sum_{k=1}^{|V|}\\exp^{u_{t,k}}}\\end{equation}\n",
        "\n",
        "## Negative Sampling\n",
        "\n",
        "So far, we have studied the basics of Skip-Gram, but there is an issue with the __original softmax objective of Skip-gram__. It is __highly computationally expensive__:\n",
        "- It requires scanning through the output-embeddings of all words in the vocabulary in order to calculate the sum from the __denominator__.\n",
        "- Typically such vocabularies contain hundreds of thousands of words.\n",
        "\n",
        "Because of this inefficiency most implementations use an alternative, _negative-sampling objective_, which rephrases the problem as a set of independent binary classification tasks.\n",
        "\n",
        "Instead of defining the complete probability distribution over words, __the model learns to distinguish the correct training pairs from incorrect pairs, which are randomly generated pairs__.\n",
        "\n",
        "For each correct pair the model draws $m$ negative ones — with $m$ being a hyperparameter. All negative samples have the same $V_t$ as the original training pair, but their $V_c$ is drawn from an arbitrary noise distribution.\n",
        "\n",
        "- Negative pair: Keep $V_t$ and sample $V_c$ from noise distribution\n",
        "\n",
        "$$\\begin{align}\\theta &= \\arg \\max_{\\theta} \\prod_{(V_t, V_c)\\in D}P(C=1|V_t, V_c)\\prod_{(V_t, V_c)\\in D'}P(C=0|V_t, V_c) \\\\\n",
        "& = \\arg \\max_{\\theta} \\prod_{(V_t, V_c)\\in D}P(C=1|V_t, V_c)\\prod_{(V_t, V_c)\\in D'}1-P(C=1|V_t, V_c)\\\\\n",
        "& = \\arg \\max_{\\theta} \\sum_{(V_t, V_c)\\in D}\\log(P(C=1|V_t, V_c)) + \\sum_{(V_t, V_c)\\in D'}\\log(1-P(C=1|V_t, V_c))\\\\\n",
        "& = \\arg \\max_{\\theta} \\sum_{(V_t, V_c)\\in D}\\log\\frac{1}{1+\\exp^{-u_{t,c}}} + \\sum_{(V_t, V_c)\\in D'}\\log \\Bigg(1-\\frac{1}{1+\\exp^{-u_{t,c}}}\\Bigg)\\\\\n",
        "& = \\arg \\max_{\\theta} \\sum_{(V_t, V_c)\\in D}\\log\\frac{1}{1+\\exp^{-u_{t,c}}} + \\sum_{(V_t, V_c)\\in D'}\\log \\frac{1}{1+\\exp^{u_{t,c}}}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "- $D$: correct pairs\n",
        "- $D'$: all negatively sampled $|D|\\times m$ pairs\n",
        "- $P(C=1|V_t, V_c)$: the probability of $(V_t, V_c)$ being a correct pair\n",
        "\n",
        "For each sample we are making a binary decision we define $P(C=1|V_t, V_c)$ using the sigmoid function\n",
        "\n",
        "Negative (context) samples are drawn from uniform distribution raised to the power of $3/4$. Why? If you play with some sample values, you'll find that, compared to the simpler equation, this one has the tendency to increase the probability for less frequent words and decrease the probability for more frequent words.\n",
        "\n",
        "$$P(w) = \\textrm{Unif}(w)^{3/4}/Z,$$\n",
        "where $Z$ is the normalization factor.\n",
        "\n",
        "Sampling-based approaches completely do away with the softmax layer. They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. __However, sampling-based approaches are only useful at training time -- during inference, the full softmax still needs to be computed to obtain a normalised probability__.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8JwOgxDPhPW"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "import pdb\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "random.seed(1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNRImtqePhRf"
      },
      "source": [
        "def getBatch(batch_size, train_data):\n",
        "    random.shuffle(train_data) # Shuffling is necessary. Why?\n",
        "    sindex = 0\n",
        "    eindex = batch_size\n",
        "    while eindex < len(train_data):\n",
        "        batch = train_data[sindex: eindex]\n",
        "        temp = eindex\n",
        "        eindex = eindex + batch_size\n",
        "        sindex = temp\n",
        "        yield batch\n",
        "\n",
        "    if eindex >= len(train_data):\n",
        "        batch = train_data[sindex:]\n",
        "        yield batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMslhRbuPhTm"
      },
      "source": [
        "def prepare_sequence(seq, word2index):\n",
        "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
        "    return torch.Tensor(idxs).type(torch.LongTensor)\n",
        "\n",
        "def prepare_word(word, word2index):\n",
        "    return torch.Tensor([word2index[word]]).type(torch.LongTensor) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctNn389ZP8sL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2447b347-e83b-4617-ff4f-891b14ea20c0"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download(\"gutenberg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA1IoyxRPhWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "16465756-5c09-4125-9032-5d636d9b20ba"
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJBdDKjTQv7n"
      },
      "source": [
        "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:500] # sampling sentences for test\n",
        "corpus = [[word.lower() for word in sent] for sent in corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyA0SxclQ4bj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "19adb005-137c-4ad1-dfb2-8d34629ee843"
      },
      "source": [
        "print(len(corpus))\n",
        "print(corpus[0], len(corpus[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'] 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv6StfKDQ9hE"
      },
      "source": [
        "# Exclude sparse words\n",
        "MIN_COUNT = 3\n",
        "word_count = Counter(flatten(corpus))\n",
        "exclude = []\n",
        "for w, c in word_count.items():\n",
        "    if c < MIN_COUNT:\n",
        "        exclude.append(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLseXp22RBq_"
      },
      "source": [
        "vocab = list(set(flatten(corpus)) - set(exclude))\n",
        "#vocab = list(set(flatten(corpus)))\n",
        "vocab.append('<UNK>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjlDOvqrRC-b"
      },
      "source": [
        "word2index = {'<UNK>' : 0}\n",
        "\n",
        "for vo in vocab:\n",
        "    if word2index.get(vo) is None:\n",
        "        word2index[vo] = len(word2index)\n",
        "\n",
        "index2word = {v:k for k, v in word2index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm1qGna9RQXn"
      },
      "source": [
        "WINDOW_SIZE = 5 # Range of contexts\n",
        "windows =  flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE,\n",
        "                                     WINDOW_SIZE * 2 + 1)) for c in corpus])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o21NTYMsRRN4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "10c1ed28-11db-489c-d6e8-00de6edec897"
      },
      "source": [
        "print(windows[0])\n",
        "print(windows[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NtOGcHtRThc"
      },
      "source": [
        "# Create Training Set\n",
        "train_data = []\n",
        "for window in windows:\n",
        "    for i in range(WINDOW_SIZE * 2 + 1):\n",
        "        if window[i] in exclude or window[WINDOW_SIZE] in exclude:\n",
        "            continue # min_count\n",
        "\n",
        "        if i == WINDOW_SIZE or window[i] == '<DUMMY>':\n",
        "            continue\n",
        "\n",
        "        # window[WINDOW_SIZE]: target word\n",
        "        # window[i]          : context word\n",
        "        train_data.append((window[WINDOW_SIZE], window[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj1MLZrIRVTv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "b387bd6f-914e-43f8-c3ad-03db3f2a36c5"
      },
      "source": [
        "# 2-Gram dataset\n",
        "train_data[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('(', 'supplied'),\n",
              " ('(', 'by'),\n",
              " ('(', 'a'),\n",
              " ('(', 'late'),\n",
              " ('supplied', '('),\n",
              " ('supplied', 'by'),\n",
              " ('supplied', 'a'),\n",
              " ('supplied', 'late'),\n",
              " ('by', '('),\n",
              " ('by', 'supplied')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_ouBZroRWgS"
      },
      "source": [
        "X_p = []\n",
        "y_p = []\n",
        "for tr in train_data:\n",
        "    X_p.append(prepare_word(tr[0], word2index).view(1, -1))\n",
        "    y_p.append(prepare_word(tr[1], word2index).view(1, -1))\n",
        "\n",
        "train_data = list(zip(X_p, y_p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkf3IF5eRXf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "a86c4caa-d4c8-42d2-d35c-49c947abad22"
      },
      "source": [
        "train_data[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([[260]]), tensor([[44]])),\n",
              " (tensor([[260]]), tensor([[292]])),\n",
              " (tensor([[260]]), tensor([[195]])),\n",
              " (tensor([[260]]), tensor([[112]])),\n",
              " (tensor([[44]]), tensor([[260]])),\n",
              " (tensor([[44]]), tensor([[292]])),\n",
              " (tensor([[44]]), tensor([[195]])),\n",
              " (tensor([[44]]), tensor([[112]])),\n",
              " (tensor([[292]]), tensor([[260]])),\n",
              " (tensor([[292]]), tensor([[44]]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-4m_yyrRZzI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "73211246-d392-4073-ff41-58b20299156c"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o5S36ZgRh73"
      },
      "source": [
        "### Unigram Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuATs91xRa7E"
      },
      "source": [
        "word_count = Counter(flatten(corpus))\n",
        "num_total_words = sum([c for w, c in word_count.items() if w not in exclude])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJYvG1XDRlIf"
      },
      "source": [
        "alpha = 3/4\n",
        "noise_dist = {key: val/num_total_words ** alpha for key, val in word_count.items() if key not in exclude}\n",
        "Z = sum(noise_dist.values())\n",
        "\n",
        "noise_dist_normalized = {key: val / Z for key, val in noise_dist.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40hA817qRmGb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e8acd33a-76d4-45dd-d3f4-87205a34588d"
      },
      "source": [
        "print(f\"Normalization factor Z:        {Z:.7}\")\n",
        "print(f'Noise distribution:            {noise_dist[\"by\"]:.6}')\n",
        "print(f'Normalized noise distribution: {noise_dist_normalized[\"by\"]:.6}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalization factor Z:        9.397142\n",
            "Noise distribution:            0.0566383\n",
            "Normalized noise distribution: 0.00602719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17UvlM9cRpUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2932d05f-5ee7-4acf-f788-f596663b7799"
      },
      "source": [
        "K = 10\n",
        "np.random.choice(list(noise_dist_normalized.keys()), size=K, p=list(noise_dist_normalized.values()), replace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['would', 'true', 'of', '.', 'be', 'town', ',', 'the', 'the', ','],\n",
              "      dtype='<U11')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blOEtPbvR0GP"
      },
      "source": [
        "## Simple Question: what is the difference between nn.Embedding() and nn.Linear()?\n",
        "\n",
        "- Word embedding doesn't assume any bias. It assumes a zero-centered distribution\n",
        "\n",
        "- Following codes represent the same network structure:\n",
        "    - `nn.Linear(vocab_size, embed_dim, bias=False)`\n",
        "    - `nn.Embedding(vocab_size, embed_dim)`\n",
        "\n",
        "- However, `nn.Embedding()` is more efficient, why?\n",
        "    - Input of `nn.Embedding()` is integer: index of one-hot-vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyltew2hRqRD"
      },
      "source": [
        "def negative_sampling(targets, noise_dist_normalized, k):\n",
        "    batch_size = targets.size(0)\n",
        "    neg_samples = []\n",
        "    for i in range(batch_size):\n",
        "        nsample = []\n",
        "        if device=='cuda':\n",
        "            # GPU -> CPU\n",
        "            target_index = targets[i].data.cpu().tolist()[0] # PyTorch Tensor -> List\n",
        "        else:\n",
        "            target_index = targets[i].data.tolist()[0]\n",
        "\n",
        "        while len(nsample) < k: # num of sampling\n",
        "            neg = np.random.choice(list(noise_dist_normalized.keys()),\n",
        "                                   size=1, p=list(noise_dist_normalized.values()))\n",
        "            neg_word = neg[0]\n",
        "            if word2index[neg_word] == target_index:\n",
        "                continue\n",
        "            nsample.append(neg_word)\n",
        "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
        "\n",
        "    return torch.cat(neg_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H37-ioHuR1rk"
      },
      "source": [
        "class SkipgramNegSampling(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(SkipgramNegSampling, self).__init__()\n",
        "        self.embedding_v = nn.Embedding(vocab_size, embed_dim) # center embedding\n",
        "        self.embedding_u = nn.Embedding(vocab_size, embed_dim) # out embedding\n",
        "        self.logsigmoid = nn.LogSigmoid()\n",
        "\n",
        "        nn.init.xavier_normal_(self.embedding_v.weight)\n",
        "        nn.init.xavier_normal_(self.embedding_u.weight)\n",
        "\n",
        "    def forward(self, center_words, target_words, negative_words):\n",
        "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
        "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
        "\n",
        "        neg_embeds = -self.embedding_u(negative_words) # B x K x D\n",
        "\n",
        "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1\n",
        "        negative_score = torch.sum(neg_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2), 1).view(negs.size(0), -1) # BxK -> Bx1\n",
        "\n",
        "        loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
        "\n",
        "        return -torch.mean(loss)\n",
        "\n",
        "    def prediction(self, inputs):\n",
        "        embeds = self.embedding_v(inputs)\n",
        "\n",
        "        return embeds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVlvr5ZuR2ag"
      },
      "source": [
        "EMBEDDING_SIZE = 30\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 30\n",
        "NEG = 10 # Num of Negative Sampling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NyBoMSvR3Lj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0956149d-4e1d-40ba-be2e-fffadb4f4688"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "losses = []\n",
        "model = SkipgramNegSampling(len(word2index), EMBEDDING_SIZE)\n",
        "if device == 'cuda':\n",
        "    model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOublOxWR37O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "dae9acb2-fe99-47ee-836d-d46f08fae045"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SkipgramNegSampling(\n",
              "  (embedding_v): Embedding(479, 30)\n",
              "  (embedding_u): Embedding(479, 30)\n",
              "  (logsigmoid): LogSigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05p4zEz-dc09"
      },
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPxzgQaQR7qk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "bdb4c045-06e3-460d-eda7-a187680b0f78"
      },
      "source": [
        "for epoch in range(EPOCH):\n",
        "    scheduler.step()\n",
        "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
        "        inputs, targets = zip(*batch)\n",
        "\n",
        "        inputs = torch.cat(inputs).to(device) # B x 1\n",
        "        targets = torch.cat(targets).to(device) # B x 1\n",
        "        negs = negative_sampling(targets, noise_dist_normalized, NEG).to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        loss = model(inputs, targets, negs)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        if i % 20 ==0:\n",
        "            lr = get_lr(optimizer)\n",
        "            print(f\"Epoch : {epoch} || Iter: {i} || Loss : {loss:.2f} || LR: {lr:.6f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0 || Iter: 0 || Loss : 1.39 || LR: 0.001000\n",
            "Epoch : 0 || Iter: 20 || Loss : 1.37 || LR: 0.001000\n",
            "Epoch : 0 || Iter: 40 || Loss : 1.32 || LR: 0.001000\n",
            "Epoch : 0 || Iter: 60 || Loss : 1.25 || LR: 0.001000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-abd1d78ef637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mnegs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_dist_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNEG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-163-1956e5e74170>\u001b[0m in \u001b[0;36mnegative_sampling\u001b[0;34m(targets, noise_dist_normalized, k)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# num of sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             neg = np.random.choice(list(noise_dist_normalized.keys()), \n\u001b[0;32m---> 14\u001b[0;31m                                    size=1, p=list(noise_dist_normalized.values()))\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mneg_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneg_word\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH9YeCdDSI5Z"
      },
      "source": [
        "def word_similarity(target, vocab):\n",
        "    target_V = model.prediction(prepare_word(target, word2index).to(device))\n",
        "\n",
        "    similarities = []\n",
        "    for i in range(len(vocab)):\n",
        "        if vocab[i] == target:\n",
        "            continue\n",
        "\n",
        "        vector = model.prediction(prepare_word(list(vocab)[i], word2index).to(device))\n",
        "\n",
        "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0]\n",
        "        similarities.append([vocab[i], cosine_sim])\n",
        "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQAlGeqhSQFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "cdebeb80-df4d-4595-8a78-5591f27cce0a"
      },
      "source": [
        "test = random.choice(list(vocab))\n",
        "print(test)\n",
        "word_similarity(test, vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ye', 0.6002415418624878],\n",
              " ['enter', 0.5219005942344666],\n",
              " ['those', 0.45959484577178955],\n",
              " ['two', 0.45800450444221497],\n",
              " ['thing', 0.4344619810581207],\n",
              " ['and', 0.43179237842559814],\n",
              " ['monstrous', 0.4307003617286682],\n",
              " ['hand', 0.4264099597930908],\n",
              " ['earth', 0.39714330434799194],\n",
              " ['stranded', 0.3869866132736206]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    }
  ]
}